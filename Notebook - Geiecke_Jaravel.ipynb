{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7649100",
   "metadata": {},
   "source": [
    "# Replication of Chopra \\& Haaland (2024)\n",
    "\n",
    "### <u>Instructions to Replicate Chopra (2024) \\& Haaland</u>\n",
    "\n",
    "#### <u>Local Version</u>\n",
    "\n",
    "#### <u>Online Version</u>\n",
    "\n",
    "\n",
    "# Replication \\& Extension of Geiecke \\& Jaravel (2025)\n",
    "\n",
    "\n",
    "### <u>Instructions to Replicate Local Version of Geiecke \\& Jaravel (2025)</u>\n",
    "\n",
    "**Note that this part of the notebook just gives merely an extented guideline how to _replicate_ the AI-interview constructed by Geiecke \\& Jaravel (2025) _locally_. Extensions are introduced in the next section.**\n",
    "\n",
    "Overall, to do the steps to replicate the AI_interview can take up to one hour from scratch.\n",
    "The instructions contain downloading the repository and creating the environment to run this Notebook.\n",
    "\n",
    "**(1)** Download the replication package from the repository of Geiecke \\& Jaravel (2025). This can be done by running\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/friedrichgeiecke/interviews.git\n",
    "cd interviews\n",
    "```\n",
    "\n",
    "in the terminal (of VS Code). This will download all the necessary file to run the robust-AI interview.\n",
    "\n",
    "**(2)** Next, install Python. Therefore, if not already installed, go to https://docs.anaconda.com/miniconda/miniconda-install/ and download and install miniconda.\n",
    "\n",
    "**(3)** An API key for the AI (LLM) that is used for the AI-led interview has to be obtained from https://platform.openai.com/ [ChatGPT] or https://www.anthropic.com/api [Claude].\n",
    "\n",
    "**(4)** In the repository folder on your computer, paste your API key into the file `/code/.streamlit/secrets.toml` (requires to make hidden folders visible).\n",
    "Moreover, in the `config.py`, select a language model and adjust the interview outline.\n",
    "\n",
    "**(5)** If (mini)conda is installed, create the environment from the .yml file by writing\n",
    "\n",
    "```bash\n",
    "conda env create -f interviewsenv.yml\n",
    "```\n",
    "\n",
    "Note this has to be run in a PowerShell Terminal (with Conda Extension), Command Prompt or alternatively in Anaconda Prompt. (In case you have problems to get the Conda Terminal Setup in VS Code, drop in the Anaconda Prompt Terminal 'code' and a VS code window with the needed Conda Terminal Setup opens.)\n",
    "The created environment installs Python and all libraries necessary to run the AI-interview platform and only needs to be created once. \n",
    "\n",
    "**(6)** Activate the environment by running\n",
    "\n",
    "```bash\n",
    "conda activate interviews\n",
    "```\n",
    "\n",
    "This environment can also be used as the kernel for this Notebook.\n",
    "\n",
    "**(7)** Start the AI-interview platform _locally_: \n",
    "\n",
    "```bash\n",
    "streamlit run interview.py\n",
    "```\n",
    "\n",
    "This will return a link for your browser where you can test the default interview or the interview with your adjusted outline _locally_.\n",
    "\n",
    "Interviews will be stored in the created `data` folder.\n",
    "\n",
    "**(8)** To stop the Streamlit app use the combination `Ctrl + C` in the Terminal.\n",
    "\n",
    "\n",
    "\n",
    "### <u>Extension: Focus Group</u>\n",
    "\n",
    "A logical extension of the method of qualitative interviews, which follow a semi-structured interview guide, is the focus group method.\n",
    "\n",
    "Focus groups are also (semi-)structured group discussions typically consisting of 6 to 10 participants who share relevant characteristics or experiences related to the research topic. Guided by a trained moderator, the group explores specific themes or questions in an open and interactive setting. The moderator uses a discussion guide to steer the conversation while allowing for spontaneous dialogue and the emergence of new ideas. Focus groups usually last between 60 and 120 minutes and are designed to uncover collective views, social dynamics, and diverse perspectives on a topic. They are particularly useful for exploring attitudes, motivations, expectations, and decision-making processes in depth.\n",
    "For more information on focus groups, see the submitted pdf-file (Deliverable).\n",
    "\n",
    "An AI-led focus group or a group with AI agents as participants can offer interesting new insights in a simple and (cost-)efficient way. An AI moderator can guide discussions without getting tired or biased, and can adjust questions on the spot based on what people say. This reduces a potential moderator bias. Furthermore, including AI agents as participants—trained on vast datasets or representing different stakeholder perspectives—can stimulate discussion by introducing diverse, data-informed viewpoints that human participants might not consider. Such hybrid formats can help explore \"what-if\" scenarios, test reactions to emerging trends, or challenge assumptions in a controlled yet dynamic environment, ultimately enriching the depth and breadth of qualitative findings.\n",
    "\n",
    "However, creating a focus group with multiple AI agents comes with several challenges and limitations. Below, a short selection of aspects that restrict the AI approach the most is provided.\n",
    "\n",
    "#### CHALLENGES\n",
    "\n",
    "**(1)** **How to determine whose turn it is to speak?**\n",
    "\n",
    "**(2)** ****\n",
    "\n",
    "**(3)**\n",
    "\n",
    "#### LIMITATIONS\n",
    "\n",
    "**(1)**\n",
    "\n",
    "**(2)**\n",
    "\n",
    "**(3)** **Integration with Human Participants:** In hybrid focus groups, balancing AI contributions with human input can be difficult and may influence how participants behave or respond.\n",
    "\n",
    "\n",
    "## Ollama Extension\n",
    "I extended the interview format by offering to run it without an API Key using Ollama, in particular the model `gemma3:27b`. Therefore, I adjusted the `config.py` file by replacing the default Open AI model `\"gpt-4o-2024-05-13\"` with the Ollama model `\"gemma3:27b\"`. Moreover, I added code to the `interview.py` file which is highlighted by `# --- BEGIN: Added Ollama option for gemma3 model ---` and `# --- END: Added Ollama option for gemma3 model ---`. \n",
    "\n",
    "Overall, the AI-led interview also runs with this open-source and local model. However, some patience is needed. Since the model is smaller, it has a larger latency compared to using OpenAI API Keys. Also the local AI has a lower quality at leading the interview. Both is not surprising and has already been highlighted by Chopra \\& Haaland (2023) and Geiecke \\& Jaravel (2025).\n",
    "\n",
    "Therefore, I recommend to use Open AI API Keys. Especially for the focus-group debate later the discussion is otherwise very slow.\n",
    "\n",
    "### Ollama Installation\n",
    "To install Ollama, and thus a local AI, go to https://ollama.com/download.\n",
    "After the installation, the command\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n",
    "makes the Ollama models also accessible for Python, for instance, in VS code.\n",
    "Then, \n",
    "```bash\n",
    "ollama pull <model_name>:<model_version>\n",
    "```\n",
    "downloads a specific model (version). Since we want to run an interview with the AI it is helpful, to download a larger model. In particular, we want to use `gemma3:27b`(17GB). To just run the model on your device, i.e., to chat with the local AI, it is enough to use the command \n",
    "```bash\n",
    "ollama run <model_name>:<model_version>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### What I added\n",
    "\n",
    "**(1)** I added more comments in both files `focusgroup.py` and `config_focusgroup.py` compared to `interview.py` and `config.py` of the normal AI interview.\n",
    "\n",
    "**(2)** I added the opprtunity to run the interview/focusgroup with an open source **Ollama** model.\n",
    "\n",
    "**(3)** I added several AI agents that simulate participants in the focus-group.\n",
    "\n",
    "**(4)** I coded a possible way how the moderator or system decides on who is allowed to talk and who responds to others' comments.\n",
    "\n",
    "**(5)** I coded a the possibility to simulate either some or all participants of the focus-group with AI agents.\n",
    "\n",
    "**(6)** I coded a way to publish the interview not only locally but also publicly.\n",
    "\n",
    "**(7)** I run a focus-group and compare the outcome of the given outline with real world examples by embedding the real world example and run a clustering exercise on both transcripts.\n",
    "\n",
    "**(8)** I extend the interview format by coding the AI-agent (interviewer) to also classify the statement of the interviewee' and pointing out interesting researc questions or based on the interview, try to find out about the interviewee's demand behavior.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398becae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Beschreibung von Environmenterstellung und wie man das Interview startet\n",
    "\n",
    "### 2. File mit Erweiterungen erstellen und verbinden.\n",
    "\n",
    "### 3. Participants erstellen, Flexibel User selber sein oder nicht\n",
    "\n",
    "This part of the notebook explains/describes the installation of the AI-led interview constructed by Geiecke \\& Jaravel (2025). Overall, this can take up to one hour from scratch.\n",
    "\n",
    "The kernel/environment used for this  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f28c6",
   "metadata": {},
   "source": [
    "\n",
    "Before we start, we set the file path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1fd02f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Absolute path\n",
    "repo_path = r\"C:/Users/torben/Documents/Bonn/Uni/Courses/Data Sciene, Machine Learning, AI/Deliverable/Geiecke_Jaravel - Clone/interviews/code\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9518",
   "metadata": {},
   "source": [
    "\n",
    "**Firstly**, we have to download the replication package from the repository. **Note that this only has to be done once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b21634f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'interviews' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/friedrichgeiecke/interviews.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db253ab3",
   "metadata": {},
   "source": [
    "**Secondly**, we have to install Python. Therefore, go to https://docs.anaconda.com/miniconda/miniconda-install/ and download and install miniconda. Of course this step can be skipped if conda is already installed.\n",
    "\n",
    "**Thirdly**, API key for the AI (LLM) that is used for the AI-led interview have to be obtained from https://platform.openai.com/ or https://www.anthropic.com/api.\n",
    "\n",
    "**Fourthly**, in the repository folder on your computer, paste your API key into the file `/code/.streamlit/secrets.toml` (requires to make hidden folders visible).\n",
    "Moreover, in the `config.py`, select a language model and adjust the interview outline.\n",
    "\n",
    "**Fifthly**, if (mini)conda is installed, create the environment from the .yml file by writing and confirming with enter (or just running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da74e181",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2054946892.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mconda env create -f interviewsenv.yml\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!cd \"C:/Users/torben/Documents/Bonn/Uni/Courses/Data Sciene, Machine Learning, AI/Deliverable/Geiecke_Jaravel - Clone/interviews/code\"\n",
    "conda env create -f interviewsenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d996a8",
   "metadata": {},
   "source": [
    "\n",
    "This has to be run in a PowerShell Terminal (with Conda Extension) or Anaconda Prompt. (In case you have problems to get the Conda Terminal Setup in VS code, drop in the Anaconda Prompt Terminal 'code' and a VS code window with the needed Conda Terminal Setup opens.)\n",
    "\n",
    "\n",
    "This environment installs Python and all libraries necessary to run the platform and only needs to be done once. \n",
    "\n",
    "**Sixthly**, activate the environment by running the PowerShell code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc424d42",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda init\n",
    "conda activate interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e4585",
   "metadata": {},
   "source": [
    "**Seventhly**, start the platform by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run interview.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855cf85",
   "metadata": {},
   "source": [
    "in the terminal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
